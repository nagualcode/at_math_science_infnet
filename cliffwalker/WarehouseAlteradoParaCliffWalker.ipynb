{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WarehouseAlteradoParaCliffWalker.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1GKqWzmeqs-"
      },
      "source": [
        "### Importando as bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfdhGGMsw1H7"
      },
      "source": [
        "#import libraries\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AdpFVfy6ya9"
      },
      "source": [
        "# Modelando ambiente como figura do cliff walk:\n",
        "environment_rows = 4\n",
        "environment_columns = 9\n",
        "num_actions = 4 #up right down left"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMBlnLfSeqtA"
      },
      "source": [
        "# Criando a tabela Q(s,a)\n",
        "q_values = np.zeros((environment_rows, environment_columns, num_actions))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z43QX_t080q3"
      },
      "source": [
        "# Definindo as ações numericamente: 0 = up, 1 = right, 2 = down, 3 = left\n",
        "actions = ['up', 'right', 'down', 'left']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X25vn4VKw2as"
      },
      "source": [
        "#### Rewards\n",
        "As recompensas serão definidas conforme a figura abaixo. Usaremos recompensas negativas (punições) para evitar que o agente ande em círculos e nunca encontre o destino desejado. Além disso, punições fará com que o agente tente encontrar o menor caminho possível, reduzindo, assim, o valor das punições (maximizando as recompensas).\n",
        "![08-warehouse-map-rewards.png](attachment:08-warehouse-map-rewards.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIJu7XsLXw62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee8a973-4645-451f-8810-3dc23facbdb7"
      },
      "source": [
        "# Criando a matriz 2D para armazenas o valor das recompensas de cada estado\n",
        "rewards = np.full((environment_rows,environment_columns),-1.)\n",
        "rewards[3,8] = 100\n",
        "rewards[3,1] = -100\n",
        "rewards[3,2] = -100\n",
        "rewards[3,3] = -100\n",
        "rewards[3,4] = -100\n",
        "rewards[3,5] = -100\n",
        "rewards[3,6] = -100\n",
        "rewards[3,7] = -100\n",
        "rewards[3,8] = 100\n",
        "\n",
        "\n",
        "  \n",
        "#print rewards matrix\n",
        "for row in rewards:\n",
        "  print(row)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
            "[-1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
            "[-1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
            "[  -1. -100. -100. -100. -100. -100. -100. -100.  100.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF0uPNb_eqtC"
      },
      "source": [
        "#### Definindo algumas funções auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnCfO5tVG0LJ"
      },
      "source": [
        "def is_terminal_state(current_row_index, current_column_index):\n",
        "    \"\"\"\n",
        "    Função que avalia se a localização é um estado terminal. Se a recompensa for -1, é um estado válido.\n",
        "    \"\"\"\n",
        "    \n",
        "    if rewards[current_row_index, current_column_index] == -1.:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "#define a function that will choose a random, non-terminal starting location\n",
        "def get_starting_location():\n",
        "    \"\"\"\n",
        "    Função que escolhe o estado inicial de cada época de treinamento de maneira aleatória.\n",
        "    \"\"\"\n",
        "    # escolhe uma linha e uma coluna aleatória\n",
        "    current_row_index = np.random.randint(environment_rows)\n",
        "    current_column_index = np.random.randint(environment_columns)\n",
        "    \n",
        "    # Verifica se é um estado válido (quadrado branco).\n",
        "    while is_terminal_state(current_row_index, current_column_index):\n",
        "        current_row_index = np.random.randint(environment_rows)\n",
        "        current_column_index = np.random.randint(environment_columns)\n",
        "\n",
        "    return current_row_index, current_column_index\n",
        "\n",
        "#define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
        "def get_next_action(current_row_index, current_column_index, epsilon):\n",
        "    \"\"\"\n",
        "    Escolhe uma ação com base no algoritmo epsilon greedy.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Se a escolha randomica entre 0 e 1 for menor que epsilon, escolhe o estado com valor mais promissor\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.argmax(q_values[current_row_index, current_column_index])\n",
        "    else: \n",
        "        # seleciona uma ação aleatória\n",
        "        return np.random.randint(4)\n",
        "\n",
        "#define a function that will get the next location based on the chosen action\n",
        "def get_next_location(current_row_index, current_column_index, action_index):\n",
        "    \"\"\"\n",
        "    Função que define a próxima localização com base na ação selecionada.\n",
        "    \"\"\"\n",
        "    \n",
        "    new_row_index = current_row_index\n",
        "    new_column_index = current_column_index\n",
        "    \n",
        "    if actions[action_index] == 'up' and current_row_index > 0:\n",
        "        new_row_index -= 1\n",
        "    elif actions[action_index] == 'right' and current_column_index < environment_columns - 1:\n",
        "        new_column_index += 1\n",
        "    elif actions[action_index] == 'down' and current_row_index < environment_rows - 1:\n",
        "        new_row_index += 1\n",
        "    elif actions[action_index] == 'left' and current_column_index > 0:\n",
        "        new_column_index -= 1\n",
        "    \n",
        "    return new_row_index, new_column_index\n",
        "\n",
        "def get_shortest_path(start_row_index, start_column_index):\n",
        "    \"\"\"\n",
        "    Função que retorna o menor caminho entre qualquer localização e a área de entrega.\n",
        "    \"\"\"\n",
        "    # Retorna vazio se for uma localização inválida\n",
        "    if is_terminal_state(start_row_index, start_column_index):\n",
        "        return []\n",
        "  \n",
        "    else: \n",
        "        # para localizaçõe válidas\n",
        "        current_row_index, current_column_index = start_row_index, start_column_index\n",
        "        shortest_path = []\n",
        "        shortest_path.append([current_row_index, current_column_index])\n",
        "\n",
        "    # continua se movendo ao longo do caminho até encontrar o objetivo\n",
        "    while not is_terminal_state(current_row_index, current_column_index):\n",
        "        # escolhe a melhor ação \n",
        "        action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
        "        \n",
        "        # move até a próxima localização no caminho, adicionando a nova localização à lista\n",
        "        current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
        "        shortest_path.append([current_row_index, current_column_index])\n",
        "\n",
        "    return shortest_path"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnRHH35OeqtD"
      },
      "source": [
        "path = get_shortest_path(3,0)\n",
        "print('Caminho até a área de entrega:\\n',path)\n",
        "\n",
        "#### Training Agent model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N5BB0m0JHIn"
      },
      "source": [
        "# Definindo os parâmetros de treinamento\n",
        "epsilon = 0.9 # percentual de vezes que irá escolher a melhor ação ao invés de uma ação aleatória\n",
        "discount_factor = 0.9 # fator de desconteo para recompensas futuras\n",
        "learning_rate = 0.9 # taxa de aprendizado\n",
        "epochs = 1000\n",
        "\n",
        "# Realizando treinamento durante 1000 episódios\n",
        "for episode in range(epochs):\n",
        "    # pega localização inicial para o episódio\n",
        "    row_index, column_index = get_starting_location()\n",
        "\n",
        "    # toma ações até que chegue a um estado terminal (área de entrega ou colisão com prateleiras)\n",
        "    while not is_terminal_state(row_index, column_index):\n",
        "        # escolhe uma ação\n",
        "        action_index = get_next_action(row_index, column_index, epsilon)\n",
        "\n",
        "        # realiza a ação, fazendo a transição para o próximo estado (vai para o próximo estado)\n",
        "        old_row_index, old_column_index = row_index, column_index # armazena a posição anterior\n",
        "        row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
        "\n",
        "        # recebe a recompensa por de deslocar para o novo estado e calcula a TD\n",
        "        reward = rewards[row_index, column_index]\n",
        "        old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
        "        temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
        "\n",
        "        # atualiza a tabela Q para o par estado-ação anterior\n",
        "        new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
        "        q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
        "\n",
        "print('Training complete!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JqQfjYdfyBh"
      },
      "source": [
        "## Selecionando o menor caminho\n",
        "Agora que o agente já foi completamente treinado, podemos verificar a qualidade do treinamento, verificando se o caminho sugerido é o menor, se chega até a área de entrega e se não há colisão com as prateleiras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1YO3mj_oS2J"
      },
      "source": [
        "# Testado para a posição inicial\n",
        "print('Origem:\\nLinha: 3 / Coluna: 0\\n',get_shortest_path(3,0)) # origem: linha 3, coluna 0\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}